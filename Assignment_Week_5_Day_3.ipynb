{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MACHINE LEARNING**"
      ],
      "metadata": {
        "id": "-F2DpsaAxcNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. a. What are the ways to avoid the problem of initialization sensitivity in the K means Algorithm?\n",
        "\n",
        "## b. How to decide the optimal number of K in the K means Algorithm?\n",
        "\n"
      ],
      "metadata": {
        "id": "4i9wiv32xixY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) There are two ways to avoid the problem of initialization sensitivity:\n",
        "Repeat K means: It basically repeats the algorithm again and again along with initializing the centroids followed by picking up the cluster which results in the small intracluster distance and large intercluster distance.\n",
        "K Means++: It is a smart centroid initialization technique.In k-means, we randomly initialized the k number of centroids while in the k-means++ algorithm, firstly we initialized 1 centroid and for other centroids, we have to ensure that the next centroids are very far from the initial centroids which result in a lower possibility of the centroid being poorly initialized\n",
        "\n",
        "b)There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster\n"
      ],
      "metadata": {
        "id": "s56FB8dJARlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. a. What is the Objective Function of k-Means?\n",
        "\n",
        "## b. How would you perform k-Means on very large datasets?"
      ],
      "metadata": {
        "id": "mgHbQniqQTUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)In K-means, the optimization criterion is to minimize the total squared error between the training samples and their representative prototypes. This is equivalent to minimizing the trace of the pooled within covariance matrix.\n",
        "\n",
        "b)K-Means Hadoop MapReduce (KM-HMR), focuses on the MapReduce implementation of standard K-means. The second approach enhances the quality of clusters to produce clusters with maximum intra-cluster and minimum inter-cluster distances for large datasets. The results of the proposed approaches show significant improvements in the efficiency of clustering in terms of execution times"
      ],
      "metadata": {
        "id": "L3e7EBnEAXU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. a. Explain the different linkage methods used in the Hierarchical Clustering Algorithm.\n",
        "\n",
        "## b. List down the pros and cons of complete and single linkages methods in the Hierarchical Clustering Algorithm."
      ],
      "metadata": {
        "id": "ifGJl8pK1MdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Single Linkage: For two clusters R and S, the single linkage returns the minimum distance between two points i and j such that i belongs to R and j belongs to S.\n",
        "\n",
        "Complete Linkage: For two clusters R and S, the complete linkage returns the maximum distance between two points i and j such that i belongs to R and j belongs to S.\n",
        "\n",
        "Average Linkage: For two clusters R and S, first for the distance between any data-point i in R and any data-point j in S and then the arithmetic mean of these distances are calculated. Average Linkage returns this value of the arithmetic mean.\n",
        "\n",
        "b)Pros of Single-linkage:\n",
        "\n",
        "This approach can differentiate between non-elliptical shapes as long as the gap between the two clusters is not small.\n",
        "Cons of Single-linkage:\n",
        "\n",
        "This approach cannot separate clusters properly if there is noise between clusters."
      ],
      "metadata": {
        "id": "ffLJ_LroBast"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. a. What is a dendrogram in Hierarchical Clustering Algorithm?\n",
        "\n",
        "## b. Explain the different parts of dendrograms in the Hierarchical Clustering Algorithm."
      ],
      "metadata": {
        "id": "he2lIQDMTC1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters\n",
        "\n",
        "b)A dendrogram is a tree-like structure that explains the relationship between all the data points in the system. However, like a regular family tree, a dendrogram need not branch out at regular intervals from top to bottom as the vertical direction (y-axis) in it represents the distance between clusters in some metric."
      ],
      "metadata": {
        "id": "9vxYKWNZBfpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. a. What is PCA? What does a PCA do?\n",
        "## b. List down the steps of a PCA algorithm."
      ],
      "metadata": {
        "id": "q7Fc1Q866FzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance.\n",
        "\n",
        "b) Get your data. \n",
        "\n",
        "Give your data a structure.\n",
        "\n",
        " Standardize your data. \n",
        "\n",
        "Get Covariance of Z.\n",
        "\n",
        " Calculate Eigen Vectors and Eigen Values.\n",
        "\n",
        "Sort the Eigen Vectors.\n",
        "\n",
        " Calculate the new features."
      ],
      "metadata": {
        "id": "N5sH7RZACfbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. a. Can PCA be used for regression-based problem statements? If Yes, then explain the scenario where we can use it.\n",
        "\n",
        "## b.  Can we use PCA for feature selection?"
      ],
      "metadata": {
        "id": "l4GBJe7iWzDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)Yes, we can use Principal Components for regression problem statements. PCA would perform well in cases when the first few Principal Components are sufficient to capture most of the variation in the independent variables as well as the relationship with the dependent variable.\n",
        "\n",
        "b)The only way PCA is a valid method of feature selection is if the most important variables are the ones that happen to have the most variation in them . However this is usually not true. As an example, imagine you want to model the probability that an NFL team makes the playoffs"
      ],
      "metadata": {
        "id": "5XFafxnMCk0k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8BirRgUf6YHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}